{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9172545",
   "metadata": {},
   "source": [
    "# 複数のベクトルを1つのドキュメントに紐付けて検索する方法\n",
    "\n",
    "1つのドキュメントに対して複数のベクトルを保存することは、多くの場面で有用です。これにはいくつかの利点や活用例があります。  \n",
    "例えば、ドキュメントの複数のチャンクを埋め込みとして生成し、それらの埋め込みを親ドキュメントに関連付けることで  \n",
    "リトリーバーがチャンクに一致した場合でも、親ドキュメント全体を返すことが可能になります。\n",
    "\n",
    "LangChainでは、このプロセスを簡略化するためにベースクラスとして`MultiVectorRetriever`を実装しています。  \n",
    "複数のベクトルを1つのドキュメントに紐付ける方法に多くの複雑さが伴いますが、このクラスがそれをシンプルにしてくれます。  \n",
    "このノートブックでは、一般的なベクトル生成方法と`MultiVectorRetriever`の使用方法について解説します。\n",
    "\n",
    "複数のベクトルを生成する方法\n",
    "1. 小さなチャンク\n",
    "    - ドキュメントを小さなチャンクに分割し、それぞれを埋め込みとして生成します。\n",
    "    - これは`ParentDocumentRetriever`が行う方法です。\n",
    "2. 要約\n",
    "    - 各ドキュメントの要約を作成し、それを埋め込みとして生成します（ドキュメントそのものの埋め込みと併用するか、代わりに利用）。\n",
    "3. 仮想的な質問\n",
    "    - 各ドキュメントが適切に回答できる仮想的な質問を作成し、それを埋め込みとして生成します（ドキュメントそのものの埋め込みと併用するか、代わりに利用）。\n",
    "\n",
    "また、埋め込みを手動で追加する方法もサポートされています。  \n",
    "これにより、特定の質問やクエリがドキュメントを返すように明示的に設定できるため、検索の制御がより柔軟になります。\n",
    "\n",
    "以下で、実例を通じて解説します。  \n",
    "まずいくつかのドキュメントを作成し、それらをOpenAI埋め込みを使用してインメモリのChromaベクトルストアにインデックス化します。  \n",
    "ただし、他のLangChainベクトルストアや埋め込みモデルを使用することも可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cecd95-3499-465a-895a-944627ffb77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet  langchain-chroma langchain langchain-openai > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18c1421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なモジュールをインポート\n",
    "from langchain.storage import InMemoryByteStore  # メモリ内でのストレージ管理\n",
    "from langchain_chroma import Chroma  # Chroma ベクトルストアのインターフェース\n",
    "from langchain_community.document_loaders import TextLoader  # テキストファイルをロードするためのローダー\n",
    "from langchain_openai import OpenAIEmbeddings  # OpenAIの埋め込みモデルを利用\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter  # ドキュメント分割ツール\n",
    "\n",
    "# 読み込むテキストファイルを指定し、ローダーを作成\n",
    "loaders = [\n",
    "    TextLoader(\"paul_graham_essay.txt\"),  # Paul Graham のエッセイ\n",
    "    TextLoader(\"state_of_the_union.txt\"),  # 一般教書演説の内容\n",
    "]\n",
    "\n",
    "# ドキュメントを格納するリストを初期化\n",
    "docs = []\n",
    "\n",
    "# 各ローダーからテキストを読み込み、docsリストに追加\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "# 大きなチャンクに分割するためのテキストスプリッターを作成\n",
    "# chunk_size=10000 で、10,000文字ごとに分割\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000)\n",
    "\n",
    "# 読み込んだドキュメントをチャンクに分割\n",
    "docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# 子チャンクをインデックス化するために使用するベクトルストアを作成\n",
    "# - \"full_documents\" というコレクション名で管理\n",
    "# - OpenAI の埋め込み関数を指定\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\",  # コレクションの名前\n",
    "    embedding_function=OpenAIEmbeddings()  # ベクトル生成に使用する埋め込みモデル\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa17beda",
   "metadata": {},
   "source": [
    "## 小さなチャンク\n",
    "\n",
    "多くの場合、大きな情報のチャンクを取得するのが有用ですが、より小さなチャンクを埋め込むことが役立つことがあります。  \n",
    "これにより、埋め込みが意味論的な意味をできるだけ正確に捉える一方で、できる限り多くのコンテキストが下流に渡されます。  \n",
    "これは `ParentDocumentRetriever` が行っている処理です。ここでは、その仕組みの詳細を説明します。\n",
    "\n",
    "ベクトルストアとドキュメントストアの区別を行います：\n",
    "\n",
    "ベクトルストア: （サブ）ドキュメントの埋め込みをインデックス化します。  \n",
    "ドキュメントストア: 「親」ドキュメントを保持し、それらに識別子を関連付けます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e7b6b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "# 親ドキュメントを保存するためのストレージ層\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# リトリーバー（最初は空の状態で作成）\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,  # ベクトルストア（子チャンクの埋め込みを格納）\n",
    "    byte_store=store,         # バイトストア（親ドキュメントを格納）\n",
    "    id_key=id_key,            # ドキュメント識別子のキー\n",
    ")\n",
    "\n",
    "# 各ドキュメントに一意のIDを生成\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4feded4-856a-4282-91c3-53aabc62e6ff",
   "metadata": {},
   "source": [
    "次に、元のドキュメントを分割して「サブ」ドキュメントを生成します。  \n",
    "この際、対応する `Document` オブジェクトの`metadata`にドキュメント識別子を格納します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d23247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小さなチャンクを作成するために使用するスプリッター\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "sub_docs = []\n",
    "for i, doc in enumerate(docs):\n",
    "    _id = doc_ids[i]  # ドキュメント識別子を取得\n",
    "    _sub_docs = child_text_splitter.split_documents([doc])  # 元のドキュメントを分割\n",
    "    for _doc in _sub_docs:\n",
    "        _doc.metadata[id_key] = _id  # メタデータにドキュメント識別子を追加\n",
    "    sub_docs.extend(_sub_docs)  # サブドキュメントをリストに追加\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0634f8-90d5-4250-981a-5257c8a6d455",
   "metadata": {},
   "source": [
    "最後に、ドキュメントをベクトルストアとドキュメントストアにインデックス化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92ed5861",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.add_documents(sub_docs)  # サブドキュメントをベクトルストアに追加\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))  # 親ドキュメントをドキュメントストアに格納"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c48c6d-850c-4317-9b6e-1ade92f2f710",
   "metadata": {},
   "source": [
    "ベクトルストアだけを使用すると、小さなチャンクが取得されます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8afed60c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.', metadata={'doc_id': '064eca46-a4c4-4789-8e3b-583f9597e54f', 'source': 'state_of_the_union.txt'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.vectorstore.similarity_search(\"justice breyer\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717097c7-61d9-4306-8625-ef8f1940c127",
   "metadata": {},
   "source": [
    "一方で、リトリーバーはより大きな親ドキュメントを返します："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c9017f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9875"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retriever.invoke(\"justice breyer\")[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdef8339-f9fa-4b3b-955f-ad9dbdf2734f",
   "metadata": {},
   "source": [
    "リトリーバーがベクトルデータベースでデフォルトで行う検索タイプは「類似性検索（similarity search）」です。  \n",
    "LangChainのベクトルストアは、「最大マージナル関連性（Max Marginal Relevance）」による検索もサポートしています。  \n",
    "この検索タイプは、リトリーバーのsearch_typeパラメータを使用して制御できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36739460-a737-4a8e-b70f-50bf8c8eaae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9875"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers.multi_vector import SearchType\n",
    "\n",
    "retriever.search_type = SearchType.mmr\n",
    "\n",
    "len(retriever.invoke(\"justice breyer\")[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a7ae0d",
   "metadata": {},
   "source": [
    "## ドキュメントに要約を関連付けて検索\n",
    "\n",
    "要約は、チャンクが何について書かれているかをより正確に抽出するのに役立ち、より良い検索結果をもたらす可能性があります。  \n",
    "ここでは、要約を作成し、それらを埋め込みに利用する方法を説明します。\n",
    "\n",
    "ドキュメントオブジェクトを受け取り、LLMを使用して要約を生成するシンプルなチェーンを構築します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1433dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLMモデルをインスタンス化\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "import uuid\n",
    "\n",
    "from langchain_core.documents import Document  # ドキュメントオブジェクトを扱うためのモジュール\n",
    "from langchain_core.output_parsers import StrOutputParser  # 出力を文字列に変換するためのパーサー\n",
    "from langchain_core.prompts import ChatPromptTemplate  # プロンプトテンプレートを扱うためのモジュール\n",
    "\n",
    "# 要約を生成するためのチェーンを構築\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}  # ドキュメントのページ内容を抽出\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")  # プロンプトテンプレートを設定\n",
    "    | llm  # LLMに渡して要約を生成\n",
    "    | StrOutputParser()  # 出力を文字列として解析\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faa9fde-1b09-4849-a815-8b2e89c30a02",
   "metadata": {},
   "source": [
    "#### ドキュメント全体に対するバッチ処理\n",
    "このチェーンを複数のドキュメントに対して並列で処理できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41a2a738",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ef599e-140b-4905-8b62-6c52cdde1852",
   "metadata": {},
   "source": [
    "次に、前と同様に `MultiVectorRetriever` を初期化し、要約をベクトルストアにインデックス化し、元のドキュメントをドキュメントストアに保持します："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ac5e4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 子チャンクをインデックス化するためのベクトルストア\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n",
    "# 親ドキュメントを保存するためのストレージ層\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "# リトリーバー（最初は空の状態で作成）\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,  # ベクトルストア\n",
    "    byte_store=store,         # ストレージ層\n",
    "    id_key=id_key,            # ドキュメント識別子\n",
    ")\n",
    "\n",
    "# 各ドキュメントに一意のIDを生成\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "# 要約をドキュメントとして作成し、メタデータにIDを追加\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "# 要約ドキュメントをベクトルストアに追加\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "# 元のドキュメントをドキュメントストアに保存\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "862ae920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要に応じて、元のチャンクをベクトルストアに追加することも可能\n",
    "# for i, doc in enumerate(docs):\n",
    "#     doc.metadata[id_key] = doc_ids[i]\n",
    "# retriever.vectorstore.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0274892-29c1-4616-9040-d23f9d537526",
   "metadata": {},
   "source": [
    "ベクトルストアにクエリを実行すると、要約が返されます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "299232d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"President Biden recently nominated Judge Ketanji Brown Jackson to serve on the United States Supreme Court, emphasizing her qualifications and broad support. The President also outlined a plan to secure the border, fix the immigration system, protect women's rights, support LGBTQ+ Americans, and advance mental health services. He highlighted the importance of bipartisan unity in passing legislation, such as the Violence Against Women Act. The President also addressed supporting veterans, particularly those impacted by exposure to burn pits, and announced plans to expand benefits for veterans with respiratory cancers. Additionally, he proposed a plan to end cancer as we know it through the Cancer Moonshot initiative. President Biden expressed optimism about the future of America and emphasized the strength of the American people in overcoming challenges.\", metadata={'doc_id': '84015b1b-980e-400a-94d8-cf95d7e079bd'})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_docs = retriever.vectorstore.similarity_search(\"justice breyer\")\n",
    "\n",
    "sub_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f77ac5-2926-4f60-aad5-b2067900dff9",
   "metadata": {},
   "source": [
    "一方、リトリーバーにクエリを実行すると、元の大きなドキュメントが返されます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4cce5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9194"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"justice breyer\")\n",
    "\n",
    "len(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097a5396",
   "metadata": {},
   "source": [
    "## 仮想的な質問\n",
    "\n",
    "LLMを使用して、特定のドキュメントに対して質問される可能性のある仮想的な質問のリストを生成することもできます。  \n",
    "これらの質問は、RAG（Retrieval-Augmented Generation）アプリケーションにおける関連するクエリと意味的に近い関係を持つ可能性があります。\n",
    "生成された質問は埋め込まれ、ドキュメントに関連付けられることで、検索性能を向上させることができます。  \n",
    "\n",
    "以下の例では、with_structured_output メソッドを使用して、LLMの出力を文字列リストとして構造化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03d85234-c33a-4a43-861d-47328e1ec2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class HypotheticalQuestions(BaseModel):\n",
    "    \"\"\"仮想的な質問を生成するクラス\"\"\"\n",
    "\n",
    "    questions: List[str] = Field(..., description=\"質問のリスト\")\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}  # ドキュメントの内容を抽出\n",
    "    # 仮想的な質問を3つ生成するよう指示（必要に応じて調整可能）\n",
    "    | ChatPromptTemplate.from_template(\n",
    "        \"Generate a list of exactly 3 hypothetical questions that the below document could be used to answer:\\n\\n{doc}\"\n",
    "    )\n",
    "    | ChatOpenAI(max_retries=0, model=\"gpt-4o\").with_structured_output(\n",
    "        HypotheticalQuestions  # 出力を構造化して仮想的な質問のリストを生成\n",
    "    )\n",
    "    | (lambda x: x.questions)  # 質問のリストを抽出\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dddc40f-62af-413c-b944-f94a5e1f2f4e",
   "metadata": {},
   "source": [
    "チェーンを単一のドキュメントに対して呼び出すと、質問のリストが出力されることを確認できます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11d30554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What impact did the IBM 1401 have on the author's early programming experiences?\",\n",
       " \"How did the transition from using the IBM 1401 to microcomputers influence the author's programming journey?\",\n",
       " \"What role did Lisp play in shaping the author's understanding and approach to AI?\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcffc572-7b20-4b77-857a-90ec360a8f7e",
   "metadata": {},
   "source": [
    "次に、このチェーンをすべてのドキュメントに対してバッチ処理し、以前と同様にベクトルストアとドキュメントストアを組み立てます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2cd6e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ドキュメントに対してチェーンをバッチ処理し、仮想的な質問を生成\n",
    "hypothetical_questions = chain.batch(docs, {\"max_concurrency\": 5})\n",
    "\n",
    "# 子チャンクをインデックス化するためのベクトルストア\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"hypo-questions\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "# 親ドキュメントを保存するためのストレージ層\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "# リトリーバー（最初は空の状態で作成）\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# 各ドキュメントに一意のIDを生成\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "# 仮想的な質問を用いてDocumentオブジェクトを生成\n",
    "question_docs = []\n",
    "for i, question_list in enumerate(hypothetical_questions):\n",
    "    question_docs.extend(\n",
    "        [Document(page_content=s, metadata={id_key: doc_ids[i]}) for s in question_list]\n",
    "    )\n",
    "\n",
    "# ベクトルストアに仮想的な質問を追加\n",
    "retriever.vectorstore.add_documents(question_docs)\n",
    "# ドキュメントストアに元のドキュメントを保存\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cba8ab-a06f-4545-85fc-cf49d0204b5e",
   "metadata": {},
   "source": [
    "ベクトルストアにクエリを実行すると、入力クエリと意味的に類似した仮想的な質問が取得されます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b442b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='What might be the potential benefits of nominating Circuit Court of Appeals Judge Ketanji Brown Jackson to the United States Supreme Court?', metadata={'doc_id': '43292b74-d1b8-4200-8a8b-ea0cb57fbcdb'}),\n",
       " Document(page_content='How might the Bipartisan Infrastructure Law impact the economic competition between the U.S. and China?', metadata={'doc_id': '66174780-d00c-4166-9791-f0069846e734'}),\n",
       " Document(page_content='What factors led to the creation of Y Combinator?', metadata={'doc_id': '72003c4e-4cc9-4f09-a787-0b541a65b38c'}),\n",
       " Document(page_content='How did the ability to publish essays online change the landscape for writers and thinkers?', metadata={'doc_id': 'e8d2c648-f245-4bcc-b8d3-14e64a164b64'})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_docs = retriever.vectorstore.similarity_search(\"justice breyer\")\n",
    "\n",
    "sub_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c32e43-5f4a-463b-a0c2-2101986f70e6",
   "metadata": {},
   "source": [
    "一方、リトリーバーにクエリを実行すると、対応する親ドキュメントが返されます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7594b24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9194"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"justice breyer\")\n",
    "len(retrieved_docs[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
